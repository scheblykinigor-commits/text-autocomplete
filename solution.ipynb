{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bfeec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\scheb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "c:\\Users\\scheb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Этап 1: Подготовка данных\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('F:/text-autocomplete/src')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_utils import prepare_dataset\n",
    "from next_token_dataset import NextTokenDataset\n",
    "from lstm_model import LSTMAutoComplete\n",
    "from lstm_train import train_model\n",
    "from eval_lstm import calculate_rouge_lstm\n",
    "from eval_transformer_pipeline import evaluate_transformer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"Этап 1: Подготовка данных\")\n",
    "train_df, val_df, test_df = prepare_dataset(\n",
    "    \"F:/text-autocomplete/data/tweets.txt\",\n",
    "    \"F:/text-autocomplete/data\"\n",
    ")\n",
    "\n",
    "print(\"Этап 2: Обучение модели LSTM\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = NextTokenDataset('F:/text-autocomplete/data/train.csv')\n",
    "val_dataset = NextTokenDataset('F:/text-autocomplete/data/val.csv')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "model = LSTMAutoComplete(\n",
    "    vocab_size=train_dataset.vocab_size,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2\n",
    ").to(device)\n",
    "\n",
    "trained_model, train_losses, val_rouge_scores = train_model(\n",
    "    model, train_loader, val_loader, train_dataset.vocab, device\n",
    ")\n",
    "\n",
    "print(\"Этап 3: Оценка модели LST\")\n",
    "test_dataset = NextTokenDataset('F:/text-autocomplete/data/test.csv')\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "rouge1_lstm, rouge2_lstm, lstm_examples = calculate_rouge_lstm(\n",
    "    trained_model, test_loader, train_dataset.vocab, device\n",
    ")\n",
    "\n",
    "print(f\"LSTM Test Results:\")\n",
    "print(f\"ROUGE-1: {rouge1_lstm:.4f}\")\n",
    "print(f\"ROUGE-2: {rouge2_lstm:.4f}\")\n",
    "\n",
    "print(\"Этап 4: Оценка модели Transformer\")\n",
    "rouge1_transformer, rouge2_transformer, transformer_examples = evaluate_transformer()\n",
    "\n",
    "print(\"Этап 5: Сравнение и выводы\")\n",
    "\n",
    "print(\"\\nСравнение результатов\")\n",
    "print(f\"LSTM Model:\")\n",
    "print(f\"  ROUGE-1: {rouge1_lstm:.4f}\")\n",
    "print(f\"  ROUGE-2: {rouge2_lstm:.4f}\")\n",
    "\n",
    "print(f\"\\nTransformer Model (distilgpt2):\")\n",
    "print(f\"  ROUGE-1: {rouge1_transformer:.4f}\")\n",
    "print(f\"  ROUGE-2: {rouge2_transformer:.4f}\")\n",
    "\n",
    "print(\"\\nПример сравнения\")\n",
    "print(\"LSTM Examples:\")\n",
    "for i, (input_tokens, pred_tokens, target_tokens) in enumerate(lstm_examples[:2]):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Input: {' '.join(input_tokens)}\")\n",
    "    print(f\"  Pred: {' '.join(pred_tokens)}\")\n",
    "    print(f\"  Target: {' '.join(target_tokens)}\")\n",
    "    print()\n",
    "\n",
    "print(\"Transformer Examples:\")\n",
    "for i, (input_tokens, pred_tokens, target_tokens) in enumerate(transformer_examples[:2]):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Input: {' '.join(input_tokens[-5:])}\")\n",
    "    print(f\"  Pred: {' '.join(pred_tokens)}\")\n",
    "    print(f\"  Target: {' '.join(target_tokens[:10])}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nРекомендации\")\n",
    "if rouge1_transformer > rouge1_lstm:\n",
    "    print(\"Модель Transformer показывает лучшие результаты по показателям ROUGE.\")\n",
    "    print(\"Рекомендация: Используйте модель Transformer, если позволяют ограничения памяти.\")\n",
    "else:\n",
    "    print(\"Модель LSTM демонстрирует конкурентные характеристики.\")\n",
    "    print(\"Рекомендация: Используйте модель LSTM для повышения эффективности использования памяти на мобильных устройствах.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
